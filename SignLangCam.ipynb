{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project V\n",
    "#### - \"We will combine our skills from Deep Learning and NLP and create our own language translator.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hmmmmmmm...... \n",
    "### It doesn't technically say it has to be a text language, does it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Which, while I'm sure they meant for the project to involve a regular translation of language, say, from French, to English, this idea of just \"make a translator\" got me thinking...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we translated sign language instead? And instead of just images, we tried to translate from live video?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking around on the internet, I was able to come across an American Sign Language CSV dataset, which was comprised of greyscaled images of the ASL alphabet.\n",
    "While it would be simple enough to translate these images to their respective letters, what if we took it a step further, and did real time video translation of ASL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - What would that look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little something like the below code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this project, we'll construct a class called SignLanguage, which will contain all the various functions required to read into python, transform the images, and return them (this is so we could run this in multiple files as scripts if we wanted. Just running it in the notebook, we can run it all together\n",
    "- Next, we'll read the samples from Kaggle into our notebook\n",
    "- Third, apply various transformations to the dataset to increase the amount of data we're working with, and so the model will (hopefully) be able to accurately validate images from video\n",
    "- Each of our images is 28x28, so we'll do the usual stretch to 784 x 1\n",
    "- Note: Dataset comes from a Kaggle competition:\n",
    "\n",
    "https://www.kaggle.com/datamunge/sign-language-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from typing import List\n",
    "import csv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "class SignLanguage(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_label_mapping():\n",
    "\n",
    "        mapping = list(range(25))\n",
    "        mapping.pop(9)\n",
    "        return mapping\n",
    "\n",
    "    @staticmethod\n",
    "    def read_label_samples(path: str):\n",
    "\n",
    "        mapping = SignLanguage.get_label_mapping()\n",
    "        labels, samples = [], []\n",
    "        with open(path) as f:\n",
    "            _ = next(f)  # skip header\n",
    "            for line in csv.reader(f):\n",
    "                label = int(line[0])\n",
    "                labels.append(mapping.index(label))\n",
    "                samples.append(list(map(int, line[1:])))\n",
    "        return labels, samples\n",
    "\n",
    "    def __init__(self,\n",
    "            path: str=\"data/sign_mnist_train.csv\",\n",
    "            mean: List[float]=[0.485],\n",
    "            std: List[float]=[0.229]):\n",
    "\n",
    "        labels, samples = SignLanguage.read_label_samples(path)\n",
    "        self._samples = np.array(samples, dtype=np.uint8).reshape((-1, 28, 28, 1))\n",
    "        self._labels = np.array(labels, dtype=np.uint8).reshape((-1, 1))\n",
    "\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(128),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.RandomResizedCrop(28, scale=(0.8, 1.2)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self._mean, std=self._std)])\n",
    "\n",
    "        return {\n",
    "            'image': transform(self._samples[idx]).float(),\n",
    "            'label': torch.from_numpy(self._labels[idx]).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll want to use our function that loads calls the data to construct training, and testing datasets\n",
    "- This whole little project uses PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(batch_size=32):\n",
    "    trainset = SignLanguage('sign_mnist_train.csv')\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    testset = SignLanguage('sign_mnist_test.csv')\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': tensor([[[[-0.2856, -0.2513, -0.2171,  ..., -0.1314, -0.1486, -0.2856],\n",
      "          [-0.2684, -0.2342, -0.1828,  ..., -0.0972, -0.1143, -0.1314],\n",
      "          [-0.2171, -0.1828, -0.1486,  ..., -0.0629, -0.0801, -0.0972],\n",
      "          ...,\n",
      "          [ 0.4337,  0.4851,  0.5364,  ...,  0.5364,  0.5022,  0.0227],\n",
      "          [ 0.4337,  0.4851,  0.5364,  ...,  0.5536,  0.5193,  0.0398],\n",
      "          [-0.5253, -0.4739, -0.0287,  ...,  0.5536,  0.5364, -0.3369]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0398,  0.1083,  0.1426,  ...,  0.3652,  0.3823,  0.0398],\n",
      "          [ 0.0741,  0.1254,  0.1768,  ...,  0.3994,  0.4166,  0.3652],\n",
      "          [ 0.1083,  0.1597,  0.2111,  ...,  0.4337,  0.4337,  0.4508],\n",
      "          ...,\n",
      "          [ 0.6563,  0.7248,  0.7248,  ...,  0.9817,  0.9817,  0.9817],\n",
      "          [ 0.6906,  0.7419,  0.7591,  ...,  0.9988,  0.9988,  0.9988],\n",
      "          [ 0.7077,  0.7591,  0.7762,  ..., -0.3369, -0.3369, -0.3369]]]]), 'label': tensor([[19.],\n",
      "        [17.]])}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    loader, _ = get_train_test(2)\n",
    "    print(next(iter(loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just importing various dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, construct a neural network class which will be able to be called at a later point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 6, 3)\n",
    "        self.conv3 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 48)\n",
    "        self.fc3 = nn.Linear(48, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's the guts of our model:\n",
    "- Runs for 15 epochs\n",
    "- Uses a learning rate scheduler, which decreases learning rate over time (see here:\n",
    "\n",
    "https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    net = Net().float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    trainloader, _ = get_train_test()\n",
    "    for epoch in range(15):  # loop over the dataset multiple times\n",
    "        train(net, criterion, optimizer, trainloader, epoch)\n",
    "        scheduler.step()\n",
    "    torch.save(net.state_dict(), \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we write a function to train the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, trainloader, epoch):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = Variable(data['image'].float())\n",
    "        labels = Variable(data['label'].long())\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels[:, 0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.6f' % (epoch, i, running_loss / (i + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, we actually train the neural network. The function keeps a running tally of our loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0,     0] loss: 3.157291\n",
      "[0,   100] loss: 3.180325\n",
      "[0,   200] loss: 3.177582\n",
      "[0,   300] loss: 3.173294\n",
      "[0,   400] loss: 3.092386\n",
      "[0,   500] loss: 2.903753\n",
      "[0,   600] loss: 2.682888\n",
      "[0,   700] loss: 2.479103\n",
      "[0,   800] loss: 2.295678\n",
      "[1,     0] loss: 1.321397\n",
      "[1,   100] loss: 0.765594\n",
      "[1,   200] loss: 0.693508\n",
      "[1,   300] loss: 0.662273\n",
      "[1,   400] loss: 0.610449\n",
      "[1,   500] loss: 0.569189\n",
      "[1,   600] loss: 0.538422\n",
      "[1,   700] loss: 0.509749\n",
      "[1,   800] loss: 0.484125\n",
      "[2,     0] loss: 0.168282\n",
      "[2,   100] loss: 0.253409\n",
      "[2,   200] loss: 0.246858\n",
      "[2,   300] loss: 0.235893\n",
      "[2,   400] loss: 0.228783\n",
      "[2,   500] loss: 0.225065\n",
      "[2,   600] loss: 0.219218\n",
      "[2,   700] loss: 0.211332\n",
      "[2,   800] loss: 0.206462\n",
      "[3,     0] loss: 0.226306\n",
      "[3,   100] loss: 0.121513\n",
      "[3,   200] loss: 0.126204\n",
      "[3,   300] loss: 0.130682\n",
      "[3,   400] loss: 0.134932\n",
      "[3,   500] loss: 0.133569\n",
      "[3,   600] loss: 0.131217\n",
      "[3,   700] loss: 0.131261\n",
      "[3,   800] loss: 0.130944\n",
      "[4,     0] loss: 0.181717\n",
      "[4,   100] loss: 0.108770\n",
      "[4,   200] loss: 0.106334\n",
      "[4,   300] loss: 0.106667\n",
      "[4,   400] loss: 0.095677\n",
      "[4,   500] loss: 0.095780\n",
      "[4,   600] loss: 0.092345\n",
      "[4,   700] loss: 0.090518\n",
      "[4,   800] loss: 0.087639\n",
      "[5,     0] loss: 0.097901\n",
      "[5,   100] loss: 0.080410\n",
      "[5,   200] loss: 0.073521\n",
      "[5,   300] loss: 0.071471\n",
      "[5,   400] loss: 0.074620\n",
      "[5,   500] loss: 0.075412\n",
      "[5,   600] loss: 0.075336\n",
      "[5,   700] loss: 0.073241\n",
      "[5,   800] loss: 0.070699\n",
      "[6,     0] loss: 0.100050\n",
      "[6,   100] loss: 0.060954\n",
      "[6,   200] loss: 0.065317\n",
      "[6,   300] loss: 0.067165\n",
      "[6,   400] loss: 0.063710\n",
      "[6,   500] loss: 0.061529\n",
      "[6,   600] loss: 0.057475\n",
      "[6,   700] loss: 0.056915\n",
      "[6,   800] loss: 0.055873\n",
      "[7,     0] loss: 0.006083\n",
      "[7,   100] loss: 0.041002\n",
      "[7,   200] loss: 0.054753\n",
      "[7,   300] loss: 0.046961\n",
      "[7,   400] loss: 0.044972\n",
      "[7,   500] loss: 0.045962\n",
      "[7,   600] loss: 0.046704\n",
      "[7,   700] loss: 0.045616\n",
      "[7,   800] loss: 0.043300\n",
      "[8,     0] loss: 0.034106\n",
      "[8,   100] loss: 0.040658\n",
      "[8,   200] loss: 0.041684\n",
      "[8,   300] loss: 0.042111\n",
      "[8,   400] loss: 0.043968\n",
      "[8,   500] loss: 0.045022\n",
      "[8,   600] loss: 0.046812\n",
      "[8,   700] loss: 0.047120\n",
      "[8,   800] loss: 0.045744\n",
      "[9,     0] loss: 0.013129\n",
      "[9,   100] loss: 0.053285\n",
      "[9,   200] loss: 0.052582\n",
      "[9,   300] loss: 0.049149\n",
      "[9,   400] loss: 0.047991\n",
      "[9,   500] loss: 0.045974\n",
      "[9,   600] loss: 0.044059\n",
      "[9,   700] loss: 0.045682\n",
      "[9,   800] loss: 0.043697\n",
      "[10,     0] loss: 0.141427\n",
      "[10,   100] loss: 0.035376\n",
      "[10,   200] loss: 0.032455\n",
      "[10,   300] loss: 0.030025\n",
      "[10,   400] loss: 0.025868\n",
      "[10,   500] loss: 0.025191\n",
      "[10,   600] loss: 0.023258\n",
      "[10,   700] loss: 0.021971\n",
      "[10,   800] loss: 0.020396\n",
      "[11,     0] loss: 0.003613\n",
      "[11,   100] loss: 0.007357\n",
      "[11,   200] loss: 0.007578\n",
      "[11,   300] loss: 0.009697\n",
      "[11,   400] loss: 0.010157\n",
      "[11,   500] loss: 0.010259\n",
      "[11,   600] loss: 0.010068\n",
      "[11,   700] loss: 0.009827\n",
      "[11,   800] loss: 0.010410\n",
      "[12,     0] loss: 0.044997\n",
      "[12,   100] loss: 0.011378\n",
      "[12,   200] loss: 0.009881\n",
      "[12,   300] loss: 0.011616\n",
      "[12,   400] loss: 0.010839\n",
      "[12,   500] loss: 0.011290\n",
      "[12,   600] loss: 0.011086\n",
      "[12,   700] loss: 0.010379\n",
      "[12,   800] loss: 0.010586\n",
      "[13,     0] loss: 0.000111\n",
      "[13,   100] loss: 0.006191\n",
      "[13,   200] loss: 0.005455\n",
      "[13,   300] loss: 0.006943\n",
      "[13,   400] loss: 0.008547\n",
      "[13,   500] loss: 0.008413\n",
      "[13,   600] loss: 0.008460\n",
      "[13,   700] loss: 0.008070\n",
      "[13,   800] loss: 0.007590\n",
      "[14,     0] loss: 0.009694\n",
      "[14,   100] loss: 0.005873\n",
      "[14,   200] loss: 0.009209\n",
      "[14,   300] loss: 0.008337\n",
      "[14,   400] loss: 0.006766\n",
      "[14,   500] loss: 0.005918\n",
      "[14,   600] loss: 0.005951\n",
      "[14,   700] loss: 0.005885\n",
      "[14,   800] loss: 0.006018\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that our model is trained, we can evaluate it on the remaining testing data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PyTorch ==========\n",
      "Training accuracy: 99.8\n",
      "Validation accuracy: 97.0\n",
      "========== ONNX ==========\n",
      "Training set accuracy: 99.8\n",
      "Validation set accuracy: 96.8\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(outputs: Variable, labels: Variable) -> float:\n",
    "    Y = labels.numpy()\n",
    "    Yhat = np.argmax(outputs, axis=1)\n",
    "    return float(np.sum(Yhat == Y))\n",
    "\n",
    "\n",
    "def batch_evaluate(\n",
    "        net: Net,\n",
    "        dataloader: torch.utils.data.DataLoader) -> float:\n",
    "    score = n = 0.0\n",
    "    for batch in dataloader:\n",
    "        n += len(batch['image'])\n",
    "        outputs = net(batch['image'])\n",
    "        if isinstance(outputs, torch.Tensor):\n",
    "            outputs = outputs.detach().numpy()\n",
    "        score += evaluate(outputs, batch['label'][:, 0])\n",
    "    return score / n\n",
    "\n",
    "\n",
    "def validate():\n",
    "    trainloader, testloader = get_train_test()\n",
    "    net = Net().float().eval()\n",
    "\n",
    "    pretrained_model = torch.load(\"checkpoint.pth\")\n",
    "    net.load_state_dict(pretrained_model)\n",
    "\n",
    "    print('=' * 10, 'PyTorch', '=' * 10)\n",
    "    train_acc = batch_evaluate(net, trainloader) * 100.\n",
    "    print('Training accuracy: %.1f' % train_acc)\n",
    "    test_acc = batch_evaluate(net, testloader) * 100.\n",
    "    print('Validation accuracy: %.1f' % test_acc)\n",
    "\n",
    "    trainloader, testloader = get_train_test_loaders(1)\n",
    "\n",
    "    # export to onnx\n",
    "    fname = \"signlanguage.onnx\"\n",
    "    dummy = torch.randn(1, 1, 28, 28)\n",
    "    torch.onnx.export(net, dummy, fname, input_names=['input'])\n",
    "\n",
    "    # check exported model\n",
    "    model = onnx.load(fname)\n",
    "    onnx.checker.check_model(model)  # check model is well-formed\n",
    "\n",
    "    # create runnable session with exported model\n",
    "    ort_sesh = ort.InferenceSession(fname)\n",
    "    net = lambda inp: ort_sesh.run(None, {'input': inp.data.numpy()})[0]\n",
    "\n",
    "    print('=' * 10, 'ONNX', '=' * 10)\n",
    "    train_acc = batch_evaluate(net, trainloader) * 100.\n",
    "    print('Training set accuracy: %.1f' % train_acc)\n",
    "    test_acc = batch_evaluate(net, testloader) * 100.\n",
    "    print('Validation set accuracy: %.1f' % test_acc)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's where it gets good. \n",
    "- The following function does a number of things.\n",
    "- The important stuff is as follows\n",
    "- Creates a list of potential answers\n",
    "- Uses ONNX to create a session with our exported model\n",
    "- Captures video input frame by frame\n",
    "- Takes each frame, and preprocesses it:\n",
    "    - Center crops it\n",
    "    - Drops it down from RBG to grey\n",
    "    - Resizes the video to match our 28x28 dataset\n",
    "    - Reshapes\n",
    "    - Puts a letter on the window based on what the model predicts\n",
    "    - Finally, runs the inputted frames against our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-51e1a19f5d9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-51e1a19f5d9f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# Capture frame-by-frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# preprocess data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "def center_crop(frame):\n",
    "    h, w, _ = frame.shape\n",
    "    start = abs(h - w) // 2\n",
    "    if h > w:\n",
    "        return frame[start: start + w]\n",
    "    return frame[:, start: start + h]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # constants\n",
    "    index_to_letter = list('ABCDEFGHIKLMNOPQRSTUVWXY')\n",
    "    mean = 0.485 * 255.\n",
    "    std = 0.229 * 255.\n",
    "\n",
    "    # create runnable session with exported model\n",
    "    ort_sesh = ort.InferenceSession(\"signlanguage.onnx\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # preprocess data\n",
    "        frame = center_crop(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        x = cv2.resize(frame, (28, 28))\n",
    "        x = (x - mean) / std\n",
    "\n",
    "        x = x.reshape(1, 1, 28, 28).astype(np.float32)\n",
    "        y = ort_sesh.run(None, {'input': x})[0]\n",
    "\n",
    "        index = np.argmax(y, axis=1)\n",
    "        letter = index_to_letter[int(index)]\n",
    "\n",
    "        cv2.putText(frame, letter, (100, 100), cv2.FONT_HERSHEY_TRIPLEX, 2.0, (0, 255, 0), thickness=2)\n",
    "        cv2.imshow(\"Sign Language Translator\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "\n",
    "#### So that's good and all, but it's not fantastic at distinguishing sign language in a way you would actually use - it's okay when you literally only have a hand onscreen\n",
    "\n",
    "#### So the next idea was to start creating my own datasets, using Labelbox, which allowed me to create both bounded boxes on videos, as well as classification\n",
    "#### As you can see from the examples, it's a pretty nifty little tool.\n",
    "#### Basicall you can draw the box, and the box stays on screen between frames. You can resize and move the box in between frames to keep up with whatever you're bounding, and can select up to 60 frames at a time to label.\n",
    "#### These labels, at which frames, and how the boxes are shaped can then be extracted to Postman.\n",
    "#### From there, you can create CSV's with the labels, and append them to the original frames, once you've extracted and greyscaled those.\n",
    "#### Unfortunately, I didn't have time to complete this addendum.\n",
    "#### As a proof of concept, this is pretty darn cool! With the above changes, I think you could really got a useful working model out of it. \n",
    "#### Of course, someone' already done this - some Australians completed a similar project for a hackathon, which does a similar real time extrapolation of Australian Sign Language, using neural networks, which is pretty darn neat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What else can we 'translate' ?\n",
    "- We could translate a whole bunch of things. Maybe facial expressions, body language, and so on.\n",
    "- Those might be useful for autistic individuals, if we could construct a real time program to translate the subtleties of human interactions, but as a proof of concept,\n",
    "it might be easier to try and construct a sign language "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
